{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperspectral Material Classification - Inference (Google Colab)\n",
    "\n",
    "Optimized for Google Colab Pro+ with A100 GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spectral scikit-learn matplotlib tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clone Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/PlugNawapong/hsi-deeplearning.git\n",
    "%cd hsi-deeplearning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pipeline_preprocess import HyperspectralPreprocessor\n",
    "from pipeline_model import create_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configuration\n",
    "\n",
    "**IMPORTANT:** All your data should be in the `DeepLearning_Plastics` folder in Google Drive:\n",
    "- Trained model: `DeepLearning_Plastics/outputs/colab/best_model.pth`\n",
    "- Inference data: `DeepLearning_Plastics/Inference_dataset1_normalized/`\n",
    "- Output will be saved to `DeepLearning_Plastics/outputs/inference/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - Base directory: DeepLearning_Plastics in Google Drive\n",
    "BASE_DIR = '/content/drive/MyDrive/DeepLearning_Plastics'\n",
    "\n",
    "CONFIG = {\n",
    "    # Model and data paths\n",
    "    'checkpoint_path': f'{BASE_DIR}/outputs/colab/best_model.pth',\n",
    "    'data_path': f'{BASE_DIR}/Inference_dataset1_normalized',\n",
    "    'output_dir': f'{BASE_DIR}/outputs/inference',\n",
    "    \n",
    "    # Inference settings - Optimized for A100\n",
    "    'batch_size': 1024  # Larger batch for inference (no gradients)\n",
    "}\n",
    "\n",
    "print('Configuration:')\n",
    "for k, v in CONFIG.items():\n",
    "    print(f'  {k}: {v}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Load checkpoint\n",
    "print(f'Loading model from: {CONFIG[\"checkpoint_path\"]}')\n",
    "checkpoint = torch.load(CONFIG['checkpoint_path'], map_location=device)\n",
    "train_config = checkpoint['config']\n",
    "\n",
    "# Create model\n",
    "model = create_model(\n",
    "    train_config['model_type'],\n",
    "    checkpoint['model_state_dict']['conv1.weight'].shape[1],\n",
    "    train_config['num_classes']\n",
    ")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f'Loaded model: {train_config[\"model_type\"]}')\n",
    "print(f'Training validation accuracy: {checkpoint[\"val_acc\"]:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Loading data from: {CONFIG[\"data_path\"]}')\n",
    "preprocessor = HyperspectralPreprocessor(CONFIG['data_path'])\n",
    "data_cube = preprocessor.load_data()\n",
    "\n",
    "print(f'Data shape: {data_cube.shape}')\n",
    "height, width, bands = data_cube.shape\n",
    "print(f'Image size: {height} x {width} pixels')\n",
    "print(f'Spectral bands: {bands}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "pixels = data_cube.reshape(-1, bands)\n",
    "predictions = np.zeros(height * width, dtype=np.int32)\n",
    "confidences = np.zeros(height * width, dtype=np.float32)\n",
    "\n",
    "# Batch inference\n",
    "batch_size = CONFIG['batch_size']\n",
    "num_batches = (len(pixels) + batch_size - 1) // batch_size\n",
    "\n",
    "print(f'Running inference on {len(pixels):,} pixels in {num_batches} batches...')\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(num_batches), desc='Inference'):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min((i + 1) * batch_size, len(pixels))\n",
    "        \n",
    "        batch = torch.FloatTensor(pixels[start_idx:end_idx]).to(device)\n",
    "        outputs = model(batch)\n",
    "        probs = torch.softmax(outputs, dim=1)\n",
    "        \n",
    "        max_probs, preds = probs.max(1)\n",
    "        predictions[start_idx:end_idx] = preds.cpu().numpy()\n",
    "        confidences[start_idx:end_idx] = max_probs.cpu().numpy()\n",
    "\n",
    "# Reshape to image\n",
    "pred_map = predictions.reshape(height, width)\n",
    "conf_map = confidences.reshape(height, width)\n",
    "\n",
    "print(f'\\nInference complete!')\n",
    "print(f'Mean confidence: {conf_map.mean():.4f}')\n",
    "print(f'Min confidence: {conf_map.min():.4f}')\n",
    "print(f'Max confidence: {conf_map.max():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Prediction map\n",
    "im1 = ax1.imshow(pred_map, cmap='tab10')\n",
    "ax1.set_title('Prediction Map', fontsize=14)\n",
    "ax1.axis('off')\n",
    "cbar1 = plt.colorbar(im1, ax=ax1)\n",
    "cbar1.set_label('Class', fontsize=12)\n",
    "\n",
    "# Confidence map\n",
    "im2 = ax2.imshow(conf_map, cmap='viridis', vmin=0, vmax=1)\n",
    "ax2.set_title('Confidence Map', fontsize=14)\n",
    "ax2.axis('off')\n",
    "cbar2 = plt.colorbar(im2, ax=ax2)\n",
    "cbar2.set_label('Confidence', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Class Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(pred_map, return_counts=True)\n",
    "\n",
    "# Bar plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1.bar(unique, counts)\n",
    "ax1.set_xlabel('Class', fontsize=12)\n",
    "ax1.set_ylabel('Pixel Count', fontsize=12)\n",
    "ax1.set_title('Predicted Class Distribution', fontsize=14)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Pie chart\n",
    "ax2.pie(counts, labels=[f'Class {i}' for i in unique], autopct='%1.1f%%')\n",
    "ax2.set_title('Class Percentage', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('\\nClass distribution:')\n",
    "for cls, cnt in zip(unique, counts):\n",
    "    print(f'  Class {cls}: {cnt:,} pixels ({100*cnt/predictions.size:.2f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = Path(CONFIG['output_dir'])\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save prediction and confidence maps\n",
    "np.save(output_dir / 'predictions.npy', pred_map)\n",
    "np.save(output_dir / 'confidences.npy', conf_map)\n",
    "print(f'Saved: {output_dir}/predictions.npy')\n",
    "print(f'Saved: {output_dir}/confidences.npy')\n",
    "\n",
    "# Save statistics\n",
    "stats = {\n",
    "    'mean_confidence': float(conf_map.mean()),\n",
    "    'min_confidence': float(conf_map.min()),\n",
    "    'max_confidence': float(conf_map.max()),\n",
    "    'image_size': {'height': int(height), 'width': int(width)},\n",
    "    'total_pixels': int(predictions.size),\n",
    "    'class_distribution': {int(k): int(v) for k, v in zip(unique, counts)}\n",
    "}\n",
    "with open(output_dir / 'statistics.json', 'w') as f:\n",
    "    json.dump(stats, f, indent=2)\n",
    "print(f'Saved: {output_dir}/statistics.json')\n",
    "\n",
    "print(f'\\nAll results saved to Google Drive: {CONFIG[\"output_dir\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Download Results (Optional)\n",
    "\n",
    "Results are already saved in Google Drive, but you can also download them directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "# Uncomment to download\n",
    "# files.download(str(output_dir / 'predictions.npy'))\n",
    "# files.download(str(output_dir / 'confidences.npy'))\n",
    "# files.download(str(output_dir / 'statistics.json'))\n",
    "\n",
    "print('Files are available in Google Drive')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}