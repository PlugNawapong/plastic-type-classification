{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# üöÄ Hyperspectral Plastic Classification Pipeline\n",
    "## Google Colab Pro+ Edition\n",
    "\n",
    "This notebook runs the complete pipeline for hyperspectral plastic classification.\n",
    "\n",
    "**Features:**\n",
    "- üéØ 6 Model Architectures (CNN, ResNet, Deep, Inception, LSTM, Transformer)\n",
    "- ‚ö° GPU Acceleration (CUDA)\n",
    "- üìä Complete Training & Inference Pipeline\n",
    "- üíæ Automatic Results Download\n",
    "\n",
    "**Requirements:**\n",
    "- Google Colab Pro+ (for best performance)\n",
    "- Data uploaded to Google Drive\n",
    "- ~20-30 GB free space on Drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## üìã Step 1: Setup & Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mount_drive"
   },
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Set working directory\n",
    "import os\n",
    "os.chdir('/content/drive/MyDrive/plastic-type-classification')\n",
    "print(\"‚úì Working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "check_gpu"
   },
   "source": [
    "## üéÆ Step 2: Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gpu_check"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "    print(\"GPU Memory:\", torch.cuda.get_device_properties(0).total_memory / 1e9, \"GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU detected! Go to Runtime > Change runtime type > GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install"
   },
   "source": [
    "## üì¶ Step 3: Install Dependencies (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_deps"
   },
   "outputs": [],
   "source": [
    "# Most packages are pre-installed in Colab, but verify\n",
    "!pip install -q scipy tqdm matplotlib pillow\n",
    "print(\"‚úì Dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "verify_data"
   },
   "source": [
    "## üìÇ Step 4: Verify Data Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "verify"
   },
   "outputs": [],
   "source": [
    "# Check required folders and files\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "required = [\n",
    "    'training_dataset/header.json',\n",
    "    'Ground_Truth/labels.png',\n",
    "    'Ground_Truth/labels.json',\n",
    "    'Inference_dataset1/header.json',\n",
    "    'run_pipeline_config.py'\n",
    "]\n",
    "\n",
    "print(\"Checking data structure...\")\n",
    "all_good = True\n",
    "for item in required:\n",
    "    exists = Path(item).exists()\n",
    "    status = \"‚úì\" if exists else \"‚úó\"\n",
    "    print(f\"{status} {item}\")\n",
    "    if not exists:\n",
    "        all_good = False\n",
    "\n",
    "# Count band files\n",
    "train_bands = len(list(Path('training_dataset').glob('ImagesStack*.png')))\n",
    "infer_bands = len(list(Path('Inference_dataset1').glob('ImagesStack*.png')))\n",
    "\n",
    "print(f\"\\n‚úì Training bands: {train_bands}\")\n",
    "print(f\"‚úì Inference bands: {infer_bands}\")\n",
    "\n",
    "if all_good and train_bands == 458 and infer_bands == 458:\n",
    "    print(\"\\n‚úÖ All data files present!\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Some files are missing. Please upload your data to Google Drive.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "config"
   },
   "source": [
    "## ‚öôÔ∏è Step 5: Configure Pipeline Parameters\n",
    "\n",
    "Edit these parameters to customize your training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "config_params"
   },
   "outputs": [],
   "source": [
    "# ==================== CONFIGURATION ====================\n",
    "\n",
    "# Mode\n",
    "MODE = \"full\"  # Options: \"full\", \"normalize\", \"train\", \"inference\"\n",
    "\n",
    "# Normalization (skip if already normalized)\n",
    "SKIP_NORMALIZE = False  # Set to True if data already normalized\n",
    "LOWER_PERCENTILE = 2\n",
    "UPPER_PERCENTILE = 98\n",
    "\n",
    "# Preprocessing\n",
    "SPECTRAL_BINNING = 2      # 2, 5, 10, or None\n",
    "SPATIAL_BINNING = None    # 2, 4, 8, or None\n",
    "WAVELENGTH_RANGE = None   # e.g., (450, 700) or None\n",
    "DENOISE = False\n",
    "DENOISE_METHOD = \"gaussian\"  # \"gaussian\" or \"median\"\n",
    "DENOISE_STRENGTH = 1.0\n",
    "\n",
    "# Model\n",
    "MODEL_TYPE = \"resnet\"  # Options: \"cnn\", \"resnet\", \"deep\", \"inception\", \"lstm\", \"transformer\"\n",
    "DROPOUT = 0.3\n",
    "\n",
    "# Training\n",
    "EPOCHS = 50\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 512  # Colab can handle larger batches\n",
    "VAL_RATIO = 0.2\n",
    "\n",
    "# ======================================================\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Mode: {MODE}\")\n",
    "print(f\"  Model: {MODEL_TYPE}\")\n",
    "print(f\"  Epochs: {EPOCHS}\")\n",
    "print(f\"  Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"  Spectral Binning: {SPECTRAL_BINNING}\")\n",
    "print(f\"  Spatial Binning: {SPATIAL_BINNING}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "run"
   },
   "source": [
    "## üöÄ Step 6: Run Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run_pipeline"
   },
   "outputs": [],
   "source": [
    "# Build command\n",
    "cmd = f\"python run_pipeline_config.py --mode {MODE}\"\n",
    "\n",
    "if SKIP_NORMALIZE:\n",
    "    cmd += \" --skip-normalize\"\n",
    "\n",
    "cmd += f\" --lower-percentile {LOWER_PERCENTILE}\"\n",
    "cmd += f\" --upper-percentile {UPPER_PERCENTILE}\"\n",
    "\n",
    "if SPECTRAL_BINNING:\n",
    "    cmd += f\" --spectral-binning {SPECTRAL_BINNING}\"\n",
    "\n",
    "if SPATIAL_BINNING:\n",
    "    cmd += f\" --spatial-binning {SPATIAL_BINNING}\"\n",
    "\n",
    "if WAVELENGTH_RANGE:\n",
    "    cmd += f\" --wavelength-range {WAVELENGTH_RANGE[0]} {WAVELENGTH_RANGE[1]}\"\n",
    "\n",
    "if DENOISE:\n",
    "    cmd += f\" --denoise --denoise-method {DENOISE_METHOD} --denoise-strength {DENOISE_STRENGTH}\"\n",
    "\n",
    "cmd += f\" --model-type {MODEL_TYPE}\"\n",
    "cmd += f\" --dropout {DROPOUT}\"\n",
    "cmd += f\" --epochs {EPOCHS}\"\n",
    "cmd += f\" --lr {LEARNING_RATE}\"\n",
    "cmd += f\" --batch-size {BATCH_SIZE}\"\n",
    "cmd += f\" --val-ratio {VAL_RATIO}\"\n",
    "\n",
    "print(\"Running command:\")\n",
    "print(cmd)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Run pipeline\n",
    "!{cmd}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "results"
   },
   "source": [
    "## üìä Step 7: View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "view_results"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "\n",
    "# Load training history\n",
    "history_path = Path('output/training/training_history.json')\n",
    "if history_path.exists():\n",
    "    with open(history_path, 'r') as f:\n",
    "        history = json.load(f)\n",
    "    \n",
    "    # Plot training curves\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    epochs = range(1, len(history['train_losses']) + 1)\n",
    "    \n",
    "    # Loss\n",
    "    ax1.plot(epochs, history['train_losses'], 'b-', label='Train Loss')\n",
    "    ax1.plot(epochs, history['val_losses'], 'r-', label='Val Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title('Training and Validation Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Accuracy\n",
    "    ax2.plot(epochs, history['train_accs'], 'b-', label='Train Acc')\n",
    "    ax2.plot(epochs, history['val_accs'], 'r-', label='Val Acc')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy (%)')\n",
    "    ax2.set_title('Training and Validation Accuracy')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nBest Validation Accuracy: {history['best_val_acc']:.2f}% (Epoch {history['best_epoch']})\")\n",
    "else:\n",
    "    print(\"Training history not found. Run training first.\")\n",
    "\n",
    "# Show inference predictions\n",
    "pred_path = Path('output/inference/predictions.png')\n",
    "if pred_path.exists():\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Inference Predictions:\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    img = Image.open(pred_path)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.title('Predicted Plastic Types')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"\\nPredictions not found. Run inference first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "stats"
   },
   "source": [
    "## üìà Step 8: View Inference Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "view_stats"
   },
   "outputs": [],
   "source": [
    "# Load inference statistics\n",
    "stats_path = Path('output/inference/inference_statistics.json')\n",
    "if stats_path.exists():\n",
    "    with open(stats_path, 'r') as f:\n",
    "        stats = json.load(f)\n",
    "    \n",
    "    print(\"Inference Statistics:\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Total pixels: {stats['total_pixels']:,}\\n\")\n",
    "    print(\"Class Distribution:\")\n",
    "    print(\"-\"*60)\n",
    "    print(f\"{'Class':<15} {'Pixels':>12} {'Percentage':>12} {'Confidence':>12}\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    for class_id, info in sorted(stats['class_distribution'].items()):\n",
    "        print(f\"{info['class_name']:<15} {info['pixel_count']:>12,} {info['percentage']:>11.2f}% {info['mean_confidence']:>11.3f}\")\n",
    "else:\n",
    "    print(\"Statistics not found. Run inference first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "download"
   },
   "source": [
    "## üíæ Step 9: Download Results (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download_results"
   },
   "outputs": [],
   "source": [
    "# Create ZIP of results\n",
    "!zip -r results.zip output/\n",
    "\n",
    "# Download to local machine\n",
    "from google.colab import files\n",
    "files.download('results.zip')\n",
    "\n",
    "print(\"‚úì Results downloaded as results.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cleanup"
   },
   "source": [
    "## üßπ Step 10: Cleanup (Optional)\n",
    "\n",
    "Remove large temporary files to free up space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cleanup_files"
   },
   "outputs": [],
   "source": [
    "# Remove normalized data (can be regenerated)\n",
    "!rm -rf training_dataset_normalized/\n",
    "!rm -rf Inference_dataset1_normalized/\n",
    "\n",
    "print(\"‚úì Normalized data removed (can be regenerated)\")\n",
    "print(\"  Model and results are preserved in output/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "quick_experiments"
   },
   "source": [
    "## üî¨ Quick Experiments: Try Different Models\n",
    "\n",
    "After normalizing once, quickly try different models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "quick_test"
   },
   "outputs": [],
   "source": [
    "# Try different models quickly (data already normalized)\n",
    "models = [\"cnn\", \"resnet\", \"deep\", \"inception\", \"transformer\"]\n",
    "\n",
    "for model in models:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training with {model.upper()} model\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    cmd = f\"python run_pipeline_config.py --mode train --skip-normalize \"\n",
    "    cmd += f\"--model-type {model} --epochs 20 --batch-size 512\"\n",
    "    \n",
    "    !{cmd}\n",
    "    \n",
    "    print(f\"\\n‚úì {model.upper()} training complete\\n\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
