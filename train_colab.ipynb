{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperspectral Material Classification - Training (Google Colab)\n",
    "\n",
    "Optimized for Google Colab Pro+ with A100 GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Check GPU and Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spectral scikit-learn matplotlib tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clone Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/PlugNawapong/hsi-deeplearning.git\n",
    "%cd hsi-deeplearning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import pipeline functions and constants\n",
    "from pipeline_preprocess import load_hyperspectral_cube, preprocess_cube, bin_spatial_labels\n",
    "from pipeline_dataset import (\n",
    "    HyperspectralDataset, create_train_val_split, create_dataloaders, load_label_image,\n",
    "    CLASS_NAMES, NUM_CLASSES  # Import class info from pipeline\n",
    ")\n",
    "from pipeline_model import create_model\n",
    "\n",
    "print('Modules imported successfully!')\n",
    "print(f'Number of classes: {NUM_CLASSES}')\n",
    "print(f'Class names: {CLASS_NAMES}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configuration\n",
    "\n",
    "**IMPORTANT:** All your data should be in the `DeepLearning_Plastics` folder in Google Drive:\n",
    "- `DeepLearning_Plastics/training_dataset_normalized/`\n",
    "- `DeepLearning_Plastics/Ground_Truth/labels.json`\n",
    "- Output will be saved to `DeepLearning_Plastics/outputs/colab/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - Base directory: DeepLearning_Plastics in Google Drive\n",
    "BASE_DIR = '/content/drive/MyDrive/DeepLearning_Plastics'\n",
    "\n",
    "CONFIG = {\n",
    "    # Data paths\n",
    "    'data_folder': f'{BASE_DIR}/training_dataset_normalized',\n",
    "    'label_path': f'{BASE_DIR}/Ground_Truth/labels.png',\n",
    "    'output_dir': f'{BASE_DIR}/outputs/colab',\n",
    "    \n",
    "    # Preprocessing (already applied if using normalized data)\n",
    "    'preprocess': {\n",
    "        'wavelength_range': None,  # Set to (450, 1000) if not already normalized\n",
    "        'spatial_binning': None,\n",
    "        'spectral_binning': None,\n",
    "    },\n",
    "    \n",
    "    # Model settings - num_classes automatically loaded from pipeline_dataset\n",
    "    'model_type': 'SpectralCNN1D',  # Options: SpectralCNN1D, HybridSN, ResNet1D, SpectralAttentionNet, DeepSpectralCNN\n",
    "    'num_classes': NUM_CLASSES,  # Automatically loaded from labels.json via pipeline_dataset\n",
    "    'dropout_rate': 0.5,\n",
    "    \n",
    "    # Training hyperparameters - Optimized for A100\n",
    "    'batch_size': 256,\n",
    "    'num_epochs': 100,\n",
    "    'learning_rate': 0.001,\n",
    "    'weight_decay': 1e-4,\n",
    "    'val_ratio': 0.2,\n",
    "    'augment': True,\n",
    "    \n",
    "    # Data loader settings - Optimized for Colab\n",
    "    'num_workers': 4,\n",
    "    'pin_memory': True,\n",
    "    \n",
    "    # Early stopping\n",
    "    'patience': 15,\n",
    "    \n",
    "    # Random seed\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "print('Configuration:')\n",
    "for k, v in CONFIG.items():\n",
    "    if k != 'preprocess':\n",
    "        print(f'  {k}: {v}')\n",
    "print(f'\\nClass names: {CLASS_NAMES}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "torch.manual_seed(CONFIG['seed'])\n",
    "np.random.seed(CONFIG['seed'])\n",
    "\n",
    "# Load hyperspectral cube\n",
    "print('Loading hyperspectral cube...')\n",
    "cube, wavelengths, header = load_hyperspectral_cube(CONFIG['data_folder'])\n",
    "print(f'Cube shape: {cube.shape}')\n",
    "\n",
    "# Preprocess if needed\n",
    "if any(CONFIG['preprocess'].values()):\n",
    "    print('Applying preprocessing...')\n",
    "    cube, wavelengths = preprocess_cube(cube, wavelengths, CONFIG['preprocess'])\n",
    "    print(f'Processed shape: {cube.shape}')\n",
    "\n",
    "print(f'Wavelength range: {wavelengths[0]:.1f} - {wavelengths[-1]:.1f} nm')\n",
    "print(f'Number of bands: {len(wavelengths)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load Labels and Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load labels\n",
    "print('Loading labels...')\n",
    "labels = load_label_image(CONFIG['label_path'])\n",
    "print(f'Labels shape: {labels.shape}')\n",
    "\n",
    "# Apply spatial binning to labels if needed\n",
    "if CONFIG['preprocess'].get('spatial_binning'):\n",
    "    bin_size = CONFIG['preprocess']['spatial_binning']\n",
    "    labels = bin_spatial_labels(labels, bin_size)\n",
    "    print(f'Applied {bin_size}x{bin_size} spatial binning to labels')\n",
    "\n",
    "# Create dataset\n",
    "print('Creating dataset...')\n",
    "dataset = HyperspectralDataset(cube, labels, augment=CONFIG['augment'])\n",
    "print(f'Total samples: {len(dataset)}')\n",
    "print(f'Number of classes: {NUM_CLASSES}')\n",
    "print(f'Classes: {CLASS_NAMES}')\n",
    "\n",
    "# Get class weights\n",
    "class_weights = dataset.get_class_weights()\n",
    "print(f'Class weights: {[f\"{w:.3f}\" for w in class_weights]}')\n",
    "\n",
    "# Split dataset\n",
    "train_dataset, val_dataset = create_train_val_split(\n",
    "    dataset, val_ratio=CONFIG['val_ratio'], random_seed=CONFIG['seed']\n",
    ")\n",
    "print(f'Train samples: {len(train_dataset)}')\n",
    "print(f'Validation samples: {len(val_dataset)}')\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader, val_loader = create_dataloaders(\n",
    "    train_dataset, val_dataset,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    num_workers=CONFIG['num_workers']\n",
    ")\n",
    "print(f'Batch size: {CONFIG[\"batch_size\"]}')\n",
    "print(f'Training batches: {len(train_loader)}')\n",
    "print(f'Validation batches: {len(val_loader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Get number of bands\n",
    "num_bands = cube.shape[2]\n",
    "\n",
    "# Create model\n",
    "model = create_model(\n",
    "    num_bands=num_bands,\n",
    "    num_classes=CONFIG['num_classes'],\n",
    "    model_type=CONFIG['model_type'],\n",
    "    dropout_rate=CONFIG['dropout_rate']\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'Model: {CONFIG[\"model_type\"]}')\n",
    "print(f'Total parameters: {total_params:,}')\n",
    "print(f'Trainable parameters: {trainable_params:,}')\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss(weight=torch.FloatTensor(class_weights).to(device))\n",
    "optimizer = optim.Adam(model.parameters(), lr=CONFIG['learning_rate'], weight_decay=CONFIG['weight_decay'])\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5, verbose=True)\n",
    "\n",
    "print('\u2713 Model created and ready for training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "best_val_acc = 0.0\n",
    "patience_counter = 0\n",
    "output_dir = Path(CONFIG['output_dir'])\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f'Starting training for {CONFIG[\"num_epochs\"]} epochs...\\n')\n",
    "\n",
    "for epoch in range(CONFIG['num_epochs']):\n",
    "    print(f'Epoch {epoch+1}/{CONFIG[\"num_epochs\"]}')\n",
    "    \n",
    "    # Training\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for spectra, labels in tqdm(train_loader, desc='Training'):\n",
    "        spectra, labels = spectra.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(spectra)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    train_loss = train_loss / len(train_loader)\n",
    "    train_acc = 100. * correct / total\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for spectra, labels in tqdm(val_loader, desc='Validation'):\n",
    "            spectra, labels = spectra.to(device), labels.to(device)\n",
    "            outputs = model(spectra)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    val_loss = val_loss / len(val_loader)\n",
    "    val_acc = 100. * correct / total\n",
    "    \n",
    "    # Update history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    \n",
    "    print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
    "    print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
    "    \n",
    "    # Scheduler\n",
    "    scheduler.step(val_acc)\n",
    "    \n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_acc': val_acc,\n",
    "            'config': CONFIG,\n",
    "            'class_names': CLASS_NAMES,\n",
    "            'num_bands': num_bands\n",
    "        }, output_dir / 'best_model.pth')\n",
    "        print(f'\u2713 Saved best model (val_acc: {val_acc:.2f}%)')\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    # Early stopping\n",
    "    if patience_counter >= CONFIG['patience']:\n",
    "        print(f'\\nEarly stopping triggered after {epoch+1} epochs')\n",
    "        break\n",
    "    \n",
    "    print()\n",
    "\n",
    "print(f'Training complete!')\n",
    "print(f'Best validation accuracy: {best_val_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Plot Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss\n",
    "ax1.plot(history['train_loss'], label='Train', linewidth=2)\n",
    "ax1.plot(history['val_loss'], label='Validation', linewidth=2)\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('Loss', fontsize=12)\n",
    "ax1.set_title('Training and Validation Loss', fontsize=14)\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "ax2.plot(history['train_acc'], label='Train', linewidth=2)\n",
    "ax2.plot(history['val_acc'], label='Validation', linewidth=2)\n",
    "ax2.set_xlabel('Epoch', fontsize=12)\n",
    "ax2.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "ax2.set_title('Training and Validation Accuracy', fontsize=14)\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'training_history.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f'Saved to: {output_dir}/training_history.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*60)\n",
    "print('TRAINING SUMMARY')\n",
    "print('='*60)\n",
    "print(f'Model: {CONFIG[\"model_type\"]}')\n",
    "print(f'Number of classes: {NUM_CLASSES}')\n",
    "print(f'Class names: {CLASS_NAMES}')\n",
    "print(f'Total epochs trained: {len(history[\"train_loss\"])}')\n",
    "print(f'Best validation accuracy: {best_val_acc:.2f}%')\n",
    "print(f'Final training accuracy: {history[\"train_acc\"][-1]:.2f}%')\n",
    "print(f'Final validation accuracy: {history[\"val_acc\"][-1]:.2f}%')\n",
    "print(f'\\nModel saved to: {output_dir}/best_model.pth')\n",
    "print(f'Training plot saved to: {output_dir}/training_history.png')\n",
    "print('='*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Download Results (Optional)\n",
    "\n",
    "Results are already saved in Google Drive, but you can also download them directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "# Uncomment to download\n",
    "# files.download(str(output_dir / 'best_model.pth'))\n",
    "# files.download(str(output_dir / 'training_history.png'))\n",
    "\n",
    "print(f'All results saved to Google Drive: {CONFIG[\"output_dir\"]}')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}